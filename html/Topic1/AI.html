
<html>
  <head>
    <title>The case for taking AI seriously as a threat to <span class="concept">humanity</span></title>
 
  </head>
  <body>
  
  
<h1>The case for taking AI seriously as a threat to <span class="concept">humanity</span></h1>

<h3>Why some people fear AI, explained.</h3>

	<figure>
	<img src="img01.webp">
	</figure>
	

<p><span class="people">Stephen Hawking</span> has said, “The development of full artificial intelligence could spell the <span class="event">end</span> of the human race.” <span class="people">Elon Musk</span>  claims that AI is <span class="concept">humanity</span>’s “<a href="https://www.vox.com/future-perfect/2018/11/2/18053418/elon-musk-artificial-intelligence-google-deepmind-openai">biggest existential threat</a>.” </p>
<p >That might have people asking: Wait, what? But these grand worries are rooted in research. Along with Hawking and Musk, prominent figures at <a href="https://dl.acm.org/citation.cfm?id=2678074">Oxford</a> and  <a href="https://www.wired.com/2015/05/artificial-intelligence-pioneer-concerns/"><span class="place">UC Berkeley</span></a> and <a href="https://ai.google/research/pubs/pub45512">many of the <span class="people">researchers</span></a> working in AI today believe that advanced AI systems, if deployed carelessly, could permanently cut off human civilization from a good <span class="event">future</span>.</p>
<p >This concern has been raised since the dawn of computing. But it has come into particular focus in recent years, as <a href="https://www.theverge.com/2017/8/2/16082272/google-mit-retouch-photos-machine-learning">advances in machine-learning techniques</a> have given us a more concrete understanding of what we can do with AI, what AI can do for (and to) us, and how much we still don’t know. </p>
<p >There are also skeptics. Some of them think advanced AI is so distant that there’s no point in thinking about it now. Others are worried that excessive hype about the power of their field might kill it prematurely. And even among the people who broadly agree that AI poses unique dangers, there are varying takes on what steps make the most sense today. </p>
<p >The conversation about AI is full of confusion, misinformation, and people talking past each other — in large part because we use the word “AI” to refer to so many things. So here’s the big picture on how artificial intelligence might pose a catastrophic danger, in nine questions:</p>

<h3>1) What is AI?</h3>

<p >Artificial intelligence is the effort to create computers capable of intelligent behavior. It is a broad catchall term, used to refer to everything from Siri to IBM’s Watson to powerful technologies we have yet to invent. </p>
<p >Some <span class="people">researchers</span> distinguish between “narrow AI” — computer systems that are better than humans in some specific, well-defined field, like playing chess or generating images or diagnosing cancer — and “general AI,” systems that can surpass human capabilities in many domains. We don’t have general AI yet, but we’re starting to get a better sense of the challenges it will pose.</p>
<p>Narrow AI has seen extraordinary <span class="concept">progress</span> over the past few years. AI systems have improved dramatically at <a href="https://www.forbes.com/sites/williamfalcon/2018/09/01/facebook-ai-just-set-a-new-record-in-translation-and-why-it-matters/#5d9112333124">translation</a>, at <a href="https://deepmind.com/blog/alphago-zero-learning-scratch/">games like chess and Go</a>, at important research biology questions like <a href="https://deepmind.com/blog/alphafold/">predicting how proteins fold</a>, and at <a href="https://medium.com/syncedreview/biggan-a-new-state-of-the-art-in-image-synthesis-cf2ec5694024">generating images</a>. AI systems determine what you’ll see in a <a href="https://www.wired.com/2016/02/ai-is-changing-the-technology-behind-google-searches/">google search</a> or in your <a href="https://www.theverge.com/2018/1/26/16937088/facebook-news-feed-explainability-ai">Facebook Newsfeed</a>. They <a href="https://openai.com/blog/musenet/">compose music</a> and <a href="https://www.vox.com/future-perfect/2019/2/14/18222270/artificial-intelligence-open-ai-natural-language-processing">write articles</a> that, at a glance, read as if a human wrote them. They play <a href="https://www.vox.com/future-perfect/2019/1/24/18196177/ai-artificial-intelligence-google-deepmind-starcraft-game">strategy</a> <a href="https://www.vox.com/2019/4/13/18309418/open-ai-dota-triumph-og">games</a>. They are being developed to <a href="https://www.businessinsider.com/the-us-army-is-developing-unmanned-drones-that-can-decide-who-to-kill-2018-4">improve drone targeting</a><strong> </strong>and <a href="https://www.reuters.com/article/us-usa-pentagon-missiles-ai-insight/deep-in-the-pentagon-a-secret-ai-program-to-find-hidden-nuclear-missiles-idusKCN1J114J">detect missiles</a>.</p>
<p >But narrow AI is <a href="http://fortune.com/ai-artificial-intelligence-deep-machine-learning/">getting less narrow</a>. Once, we made <span class="concept">progress</span> in AI by painstakingly teaching computer systems specific concept. To do computer vision — allowing a computer to<strong> </strong>identify things in pictures and video — <span class="people">researchers</span> wrote algorithms for detecting edges. To play chess, they programmed in heuristics about chess. To do natural language processing (speech recognition, transcription, translation, etc.), they drew on the field of linguistics. </p>
<p >But recently, we’ve gotten better at creating computer systems that have generalized learning capabilities. Instead of mathematically describing detailed features of a problem, we let the computer system learn that by itself. While once we treated computer vision as a completely different problem from natural language processing or platform game playing, now we can solve all three problems with the <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">same approaches</a>. </p>
<p >And as computers get good enough at narrow AI tasks, they start to exhibit more general capabilities. For example, OpenAI’s famous <a href="https://www.vox.com/future-perfect/2019/2/14/18222270/artificial-intelligence-open-ai-natural-language-processing">GPT-series</a> of text AIs is, in one sense, the narrowest of narrow AIs — it just predicts what the next word will be in a text, based on the previous words and its corpus of human language. And yet, it can now identify questions as reasonable or unreasonable and discuss the physical world (for example, answering questions about which objects are larger or which steps in a process must come first). In order to be very good at the narrow task of text <span class="concept">prediction</span>, an <span class="concept">AI</span> system will <a href="https://www.vox.com/future-perfect/21355768/gpt-3-ai-openai-turing-test-language">eventually <span class="concept">develop</span> abilities that are not narrow at all.</a></p>
<p> <span  class="opinion" >Our <span class="concept">AI</span> <span class="concept">progress</span> so far has enabled enormous advances — and has also raised urgent ethical questions </span>. When you train a computer system to predict which convicted felons will reoffend, you’re using inputs from a criminal justice system biased against black people and low-income people — and so its <a href="https://www.vox.com/science-and-health/2017/4/17/15322378/how-artificial-intelligence-learns-how-to-be-racist">outputs will likely be biased against black and low-income people too</a>.<strong> </strong><a href="https://www.vox.com/technology/2018/10/1/17882340/how-algorithms-control-your-life-hannah-fry">Making websites more addictive</a> can be great for your revenue but bad for your users. Releasing a program that <a href="https://www.vox.com/future-perfect/2019/2/14/18222270/artificial-intelligence-open-ai-natural-language-processing">writes convincing fake reviews</a> or fake news might make those widespread, making it harder for the truth to get out.</p>
<p ><a href="https://becominghuman.ai/keeping-ai-safe-and-beneficial-for-humanity-4d0416300dfa?fbclid=IwAR3or4B3Jf5s1AwEN56AwFYtQierxPYFvmGco5tUd2q0s56XRWR0fxaKvQ4">Rosie Campbell at <span class="place">UC Berkeley</span>’s Center for Human-Compatible <span class="concept">AI</span> argues</a> that these are examples, writ small, of the big worry experts have about general <span class="concept">AI</span> in the future. The difficulties we’re wrestling with today with narrow <span class="concept">AI</span> don’t come from the systems turning on us or wanting revenge or considering us inferior. Rather, they come from the disconnect between what we tell our systems to do and what we actually want them to do. </p>
<p >For example, we tell a system to run up a high score in a video game. We want it to play the game fairly and learn game skills — but if it instead has the chance to directly hack the scoring system, it will do that. It’s doing great by the metric we gave it. But we aren’t getting what we wanted. </p>
  
	
	
<figure>
	<img src="img02.webp">
	</figure>



<p >In other words, our problems come from the systems being really good at achieving the goal they learned to pursue; it’s just that the goal they learned in their training environment isn’t the outcome we actually wanted. And we’re building systems we don’t understand, which means we can’t always anticipate their behavior. </p>
<p><span  class="opinion">Right now the <span class="concept">harm</span> is limited because the systems are so limited. But it’s a pattern that could have even graver consequences for human beings in the <span class="event">future</span> as <span class="concept">AI</span> systems become more advanced.</span> </p>

<h3>2) Is it even possible to make a computer as smart as a person? </h3>

<p >Yes, though current <span class="concept">AI</span> systems aren’t nearly that smart.</p>
<p>One popular adage about <span class="concept">AI</span> is “<a href="https://en.wikipedia.org/wiki/Moravec%27s_paradox">everything that’s easy is hard, and everything that’s hard is easy</a>.” Doing complex calculations in the blink of an eye? Easy. Looking at a picture and telling you whether it’s a dog? Hard (until very recently). </p>
<p>Lots of things humans do are still outside <span class="concept">AI</span>’s grasp. For instance, it’s hard to design an <span class="concept">AI</span> system that explores an unfamiliar environment, that can navigate its way from, say, the entryway of a building it’s never been in before up the stairs to a specific person’s desk. We are just beginning to learn how to design an <span class="concept">AI</span> system that reads a book and retains an understanding of the concept.</p>
<p>The paradigm that has driven many of the biggest breakthroughs in <span class="concept">AI</span> recently is called “deep learning.” Deep learning systems can do some astonishing stuff: beat games we thought humans might never lose, invent compelling and realistic photographs, solve open problems in molecular biology. </p>
<p>These breakthroughs have made some <span class="people">researchers</span> conclude it’s <span class="concept">time</span> to start thinking about the dangers of more powerful systems, but <a href="http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/">skeptics</a> remain. The field’s pessimists argue that programs still need an extraordinary pool of structured data to learn from, require carefully chosen parameters, or work only in environments designed to avoid the problems we don’t yet know how to solve. They point to <a href="https://www.theverge.com/2018/7/3/17530232/self-driving-ai-winter-full-autonomy-waymo-tesla-uber">self-driving cars</a>, which are still mediocre under the best conditions despite the billions that have been poured into making them work. </p>
<p>It’s rare, though, to find a top researcher in <span class="concept">AI</span> who thinks that general <span class="concept">AI</span> is impossible. Instead, the field’s luminaries tend to say that it <a href="https://www.vox.com/future-perfect/2019/3/2/18244299/possible-minds-architects-intelligence-ai-experts">will happen someday</a> — but probably a day that’s a long way off.</p>
<p>Other <span class="people">researchers</span> argue that the day may not be so distant after all. </p>
<p>That’s because for almost all the history of <span class="concept">AI</span>, we’ve been held back in large part by not having enough computing power to realize our ideas fully. Many of the breakthroughs of recent years — <span class="concept">AI</span> systems that learned how to play <a href="https://www.vox.com/future-perfect/2019/1/24/18196177/ai-artificial-intelligence-google-deepmind-starcraft-game">strategy games</a>, <a href="https://www.youtube.com/watch?v=G06dEcZ-QTg">generate fake photos of celebrities</a>, <a href="https://deepmind.com/blog/alphafold/">fold proteins</a>, and <a href="https://www.theverge.com/2018/8/28/17787610/openai-dota-2-bots-ai-lost-international-reinforcement-learning">compete in massive multiplayer online strategy games</a> — have happened because that’s no longer true. Lots of algorithms that seemed not to work at all turned out to work quite well once we could run them with more computing power. </p>
<p >And the cost of a unit of computing <span class="concept">time</span> <a href="http://mediangroup.org/gpu.html">keeps falling</a>. <span class="concept">progress</span> in computing speed has slowed recently, but the cost of computing power is still estimated to be falling by a factor of 10 every 10 years. Through most of its<strong> </strong>history, <span class="concept">AI</span> has had access to less computing power than the human brain. That’s changing. By <a href="https://www.eetimes.com/author.asp?section_id=36&amp;doc_id=1322543">most estimates</a>, we’re now <a href="https://www.motherjones.com/media/2013/05/robots-artificial-intelligence-jobs-automation/">approaching the era</a> when <span class="concept">AI</span> systems can have the computing resources that we humans enjoy. </p>
<p >And deep learning, unlike previous approaches to <span class="concept">AI</span>, is highly suited to developing general capabilities.</p>
<p>“If you go back in history,” top <span class="concept">AI</span> researcher and OpenAI cofounder Ilya Sutskever <a href="https://www.vox.com/future-perfect/2019/4/17/18301070/openai-greg-brockman-ilya-sutskever">told me</a>, “they made a lot of cool demos with little symbolic <span class="concept">AI</span>. They could never scale them up — they were never able to get them to solve non-toy problems. Now with <a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment">deep learning</a> the situation is reversed. ... Not only is [the <span class="concept">AI</span> we’re developing]<strong> </strong>general, it’s also competent — if you want to get the best results on many hard problems, you must use deep learning. And it’s scalable.” </p>
<p>In other words, we didn’t need to worry about general <span class="concept">AI</span> back when winning at chess required entirely different techniques than winning at Go. But now, the same approach produces <a href="https://www.vox.com/future-perfect/2019/2/14/18222270/artificial-intelligence-open-ai-natural-language-processing">fake news</a> or <a href="https://openai.com/blog/musenet/">music</a> depending on what training data it is fed. And as far as we can discover, the programs just keep getting better at what they do when they’re allowed more computation <span class="concept">time</span> — we haven’t discovered a limit to how good they can get. Deep learning approaches to most problems blew past all other approaches when deep learning was first discovered. </p>
<p >Furthermore, breakthroughs in a field can often surprise even other <span class="people">researchers</span> in the field. “Some have argued that there is no conceivable risk to <span class="concept"><span class="concept">humanity</span></span> [from <span class="concept">AI</span>] for centuries to come,” <a href="https://www.edge.org/response-detail/26157">wrote</a> <span class="place">UC Berkeley</span> professor Stuart Russell, “perhaps forgetting that the interval of <span class="concept">time</span> between Rutherford’s confident assertion that <span class="concept">atomic</span> energy would never be feasibly extracted and Szilárd’s invention of the neutron-induced <span class="concept">nuclear</span> chain reaction was less than twenty-four hours.”</p>

<p >There’s another consideration. Imagine an <span class="concept">AI</span> that is inferior to humans at everything, with one exception: It’s a competent engineer that can build <span class="concept">AI</span> systems very effectively. Machine learning engineers who work on automating jobs in other fields often observe, humorously, that in some respects, their own field looks like one where <a href="https://en.wikipedia.org/wiki/Automated_machine_learning">much of the work</a> — the tedious tuning of parameters — could be automated. </p>
<p >If we can design such a system, then we can use its result — a better engineering <span class="concept">AI</span> — to build another, even better <span class="concept">AI</span>. This is the mind-bending scenario experts call “recursive self-improvement,” where gains in <span class="concept">AI</span> capabilities enable more gains in <span class="concept">AI</span> capabilities, allowing a system that started out behind us to rapidly <span class="event">end</span> up with abilities well beyond what we anticipated.</p>
<p >This is a possibility that has been anticipated since the first computers. I.J. Good, a colleague of Alan Turing who worked at the Bletchley Park codebreaking operation during World War II and helped build the first computers afterward, may have been the first to spell it out, <a href="https://www.sciencedirect.com/science/article/pii/S0065245808604180">back in 1965</a>: “An ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.”</p>

<h3>3) How exactly could <span class="concept">AI</span> wipe us out?</h3>

<p>It’s immediately clear <a href="https://www.vox.com/future-perfect/2018/10/19/17873822/nuclear-war-weapons-bombs-how-kill">how <span class="concept">nuclear</span> bombs will kill us</a>. No one working on mitigating <span class="concept">nuclear</span> risk has to start by explaining why it’d be a bad thing if we had a <span class="concept">nuclear</span> war. </p>
<p>The case that <span class="concept">AI</span> could pose an existential risk to humanity is more complicated and harder to grasp. So many of the people who are working to build safe <span class="concept">AI</span> systems have to start by explaining why <span class="concept">AI</span> systems, by default, are dangerous.</p>
  
<figure>
	<img src="img03.webp">
	</figure>


<p >The idea that <span class="concept">AI</span> can become a danger is rooted in the fact that <span class="concept">AI</span> systems pursue their goals, whether or not those goals are what we really intended — and whether or not we’re in the way. “You’re probably not an evil ant-hater who steps on ants out of malice,” <a href="https://www.vox.com/future-perfect/2018/10/16/17978596/stephen-hawking-ai-climate-change-robots-future-universe-earth"><span class="people">Stephen Hawking</span>  wrote,</a> “but if you’re in charge of a hydroelectric green-energy project and there’s an anthill in the region to be flooded, too bad for the ants. Let’s not place <span class="concept">humanity</span> in the position of those ants.” </p>
<p >Here’s one scenario that keeps experts up at night: We <span class="concept">develop</span> a sophisticated <span class="concept">AI</span> system with the goal of, say, estimating some number with high confidence. The <span class="concept">AI</span> realizes it can achieve more confidence in its calculation if it uses all the world’s computing hardware, and it realizes that releasing a biological superweapon to wipe out <span class="concept">humanity</span> would allow it free use of all the hardware. Having exterminated <span class="concept">humanity</span>, it then calculates the number with higher confidence.</p>
<p>It is easy to design an <span class="concept">AI</span> that averts that specific pitfall. But there are lots of ways that unleashing powerful computer systems will have unexpected and potentially devastating effects, and avoiding all of them is a much harder problem than avoiding any specific one.</p>
<p >Victoria Krakovna, an <span class="concept">AI</span> researcher at DeepMind (now a division of Alphabet, <span class="place">Google</span>’s parent company), <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml">compiled a list of examples of “specification gaming”</a>: the computer doing what we told it to do but not what we wanted it to do. For example, we tried to teach <span class="concept">AI</span> organisms in a simulation to jump, but we did it by teaching them to measure how far their “feet” rose above the ground. Instead of jumping, they<a href="http://artax.karlin.mff.cuni.cz/~krcap1am/ero/doc/krcah-ices08.pdf"> learned to grow into tall vertical poles and do flips</a> — they excelled at what we were measuring, but they didn’t do what we wanted them to do.</p>
<p>An <span class="concept">AI</span> playing the Atari exploration game <a href="https://en.wikipedia.org/wiki/Montezuma%27s_Revenge_(video_game)"><em>Montezuma’s Revenge</em></a> found a bug that <a href="https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/">let it force a key in the game to reappear</a>, thereby allowing it to earn a higher score by exploiting the glitch. An <span class="concept">AI</span> playing a different game realized it could get more points by <a href="http://aliciapatterson.org/stories/eurisko-computer-mind-its-own">falsely inserting its name as the owner of high-value items</a>. </p>
<p >Sometimes, the <span class="people">researchers</span> <a href="https://arxiv.org/pdf/1802.08842.pdf">didn’t even know how their <span class="concept">AI</span> system<strong> </strong>cheated</a>: “the agent discovers an in-game bug. ... For a reason unknown to us, the game does not advance to the second round but the platforms start to blink and the agent quickly gains a huge amount of points (close to 1 million for our episode <span class="concept">time</span> limit).” </p>
<p >What these examples make clear is that in any system that might have bugs or unintended behavior or behavior humans don’t fully understand, a sufficiently powerful <span class="concept">AI</span> system might act unpredictably — pursuing its goals through an avenue that isn’t the one we expected.</p>
<p >In his 2009 paper <a href="https://omohundro.files.wordpress.com/2009/12/ai_drives_final.pdf">“The Basic <span class="concept">AI</span> Drives,”</a> <a href="https://steveomohundro.com/author/">Steve Omohundro</a>, who has worked as a computer <span class="concept">science</span> professor at the University of Illinois Urbana-Champaign and as the president of Possibility Research, argues that almost any <span class="concept">AI</span> system will predictably try to accumulate more resources, become more efficient, and resist being turned off or modified: “These potentially harmful behaviors will occur not because they were programmed in at the start, but because of the intrinsic nature of goal driven systems.” </p>
<p>His argument goes like this: Because AIs have goals, they’ll be motivated to take actions that they can predict will advance their goals. An <span class="concept">AI</span> playing a chess game will be motivated to take an opponent’s piece and advance the board to a state that looks more winnable. </p>
<p >But the same <span class="concept">AI</span>, if it sees a way to improve its own chess evaluation algorithm so it can evaluate potential moves faster, will do that too, for the same reason: It’s just another step that advances its goal. </p>
<p >If the <span class="concept">AI</span> sees a way to harness more computing power so it can consider more moves in the <span class="concept">time</span> available, it will do that. And if the <span class="concept">AI</span> detects that someone is trying to turn off its computer mid-game, and it has a way to disrupt that, it’ll do it. It’s not that we would instruct the <span class="concept">AI</span> to do things like that; it’s that whatever goal a system has, actions like these will often be part of the best path to achieve that goal. </p>
<p >That means that any goal, even innocuous ones like playing chess or generating advertisements that get lots of clicks online, could produce unintended<strong> </strong>results if the agent pursuing it has enough intelligence and optimization power to identify weird, unexpected routes to achieve its goals. </p>
<p >Goal-driven systems won’t wake up one day with hostility to humans lurking in their hearts. But they will take actions that they predict will help them achieve their goal — even if we’d find those actions problematic, even horrifying. They’ll work to preserve themselves, accumulate more resources, and become more efficient. They already do that, but it takes the form of weird glitches in games. As they grow more sophisticated, <span class="people">scientists</span> like Omohundro predict more adversarial behavior.</p>

<h3>4) When did <span class="people">scientists</span> first start worrying about <span class="concept">AI</span> risk? </h3>

<p ><span class="people">Scientists</span> have been thinking about the potential of artificial intelligence since the early days of computers. In the <a href="https://aperiodical.com/wp-content/uploads/2018/01/Turing-Can-Computers-Think.pdf">famous paper where he put forth the Turing test</a> for determining if an artificial system is truly “intelligent,” Alan Turing wrote: </p>
<blockquote><p>Let us now assume, for the sake of argument, that these machines are a genuine possibility, and look at the consequences of constructing them. ... There would be plenty to do in trying to keep one’s intelligence up to the standards set by the machines, for it seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers. … At some stage therefore we should have to expect the machines to take control.</p></blockquote>
<p >I.J. Good worked closely with Turing and reached the same conclusions, <a href="https://io9.gizmodo.com/why-a-superintelligent-machine-may-be-the-last-thing-we-1440091472">according to his assistant, Leslie Pendleton</a>. In an excerpt from unpublished notes Good wrote shortly before he died in 2009, he writes about himself in third person and notes a disagreement with his younger self — while as a younger man, he thought powerful AIs might be helpful to us, the older Good expected <span class="concept">AI</span> to annihilate us. </p>
<blockquote><p>[The paper] “Speculations Concerning the First Ultra-intelligent Machine” (1965) ... began: “The survival of man depends on the early construction of an ultra-intelligent machine.” Those were his words during the Cold War, and he now suspects that “survival” should be replaced by “extinction.” He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that “probably Man will construct the deus ex ma<span class="place">China</span> in his own image.”</p></blockquote>
<p>In the 21st century, with computers quickly establishing themselves as a transformative force in our world, younger <span class="people">researchers</span> started expressing similar worries. </p>
<p >Nick Bostrom is a <a href="https://nickbostrom.com/">professor at the University of Oxford, the director of the <span class="event">Future</span> of <span class="concept">humanity</span> Institute, and the director of the Governance of Artificial Intelligence Program</a>. He researches <a href="https://www.vox.com/future-perfect/2018/11/19/18097663/nick-bostrom-vulnerable-world-global-catastrophic-risks">risks to <span class="concept">humanity</span></a>, both <a href="https://nickbostrom.com/">in the abstract</a> — asking questions like why we seem to be alone in the universe — and in concrete terms, analyzing the technological advances on the table and whether they endanger us. <span class="concept">AI</span>, he concluded, endangers us. </p>
<p >In 2014, he <a href="https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742?ots=1&amp;slotNum=1&amp;imprToken=d2c0c12a-b3d6-8de3-881&amp;ascsubtag=[]vx[p]17890617[t]w[r]google.com[d]D" data-cdata="{&quot;rewritten_url&quot;:&quot;https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742?ascsubtag=[]vx[p]17890617[m]m-placeholder[s]s-placeholder[t]w[c]c-placeholder[r]r-placeholder[d]d-placeholder&quot;,&quot;subtag_max_length&quot;:99,&quot;subtag_delim_length&quot;:2,&quot;subtag_key&quot;:&quot;ascsubtag&quot;,&quot;subtag_data&quot;:{&quot;ascsubtag&quot;:&quot;[]vx[p]17890617[m]m-placeholder[s]s-placeholder[t]w[c]c-placeholder[r]r-placeholder[d]d-placeholder&quot;},&quot;encode_subtag&quot;:false}">wrote a book</a> explaining the risks <span class="concept">AI</span> poses and the necessity of getting it right the first <span class="concept">time</span>, concluding, “once unfriendly superintelligence exists, it would prevent us from replacing it or changing its preferences. Our fate would be sealed.”</p>
  
  
<figure>
	<img src="img04.webp">
	</figure>

<p >Across the world, others have reached the same conclusion. Bostrom co-authored a <a href="https://nickbostrom.com/ethics/artificial-intelligence.pdf">paper on the ethics of artificial intelligence</a> with Eliezer Yudkowsky, founder of and research fellow at the Berkeley Machine Intelligence Research Institute (MIRI), an organization that works on better formal characterizations of the <span class="concept">AI</span> safety problem. </p>
<p >Yudkowsky started his career in <span class="concept">AI</span> by <a href="https://www.lesswrong.com/posts/TtYuY2QBug3dn2wuo/the-problem-with-aixi">worriedly poking holes in others’ proposals for how to make <span class="concept">AI</span> systems safe</a>, and has spent most of it working to persuade his peers that <span class="concept">AI</span> systems will, by default, be unaligned with human values (not necessarily opposed to but indifferent to human morality)<strong> </strong>— and that it’ll be a challenging technical problem to prevent that outcome.</p>
<p>Increasingly, <span class="people">researchers</span> realized that there’d be challenges that hadn’t been present with <span class="concept">AI</span> systems when they were simple. “‘Side effects’ are much more likely to occur in a complex environment, and an agent may need to be quite sophisticated to hack its reward function in a dangerous way. This may explain why these problems have received so little study in the past, while also suggesting their importance in the <span class="event">future</span>,” concluded a 2016 <a href="https://arxiv.org/pdf/1606.06565.pdf">research paper</a> on problems in <span class="concept">AI</span> safety.</p>
<p >Bostrom’s book <em>Superintelligence</em> was compelling to many people, but there were skeptics. “<a href="https://www.technologyreview.com/s/602410/no-the-experts-dont-think-superintelligent-ai-is-a-threat-to-humanity/">No, experts don’t think superintelligent <span class="concept">AI</span> is a threat to <span class="concept">humanity</span></a>,” argued an op-ed by Oren Etzioni, a professor of computer <span class="concept">science</span> at the University of Washington and CEO of the Allan Institute for Artificial Intelligence. “<a href="https://www.technologyreview.com/s/602776/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/">Yes, we are worried about the existential risk of artificial intelligence</a>,” replied a dueling op-ed by Stuart Russell, an <span class="concept">AI</span> pioneer and <span class="place">UC Berkeley</span> professor, and Allan DaFoe, a senior research fellow at Oxford and director of the Governance of <span class="concept">AI</span> program there. </p>
<p >It’s tempting to conclude that there’s a pitched battle between <span class="concept">AI</span>-risk skeptics and <span class="concept">AI</span>-risk believers. In reality, they might not disagree as profoundly as you would think.</p>
<p >Facebook’s chief <span class="concept">AI</span> scientist Yann LeCun, for example, is a vocal voice on the skeptical side. But while he <a href="https://www.popsci.com/bill-gates-fears-ai-ai-researchers-know-better">argues we shouldn’t fear <span class="concept">AI</span>, he still believes we ought to have people working on, and thinking about, <span class="concept">AI</span> safety</a>. “Even if the risk of an A.I. uprising is very unlikely and very far in the <span class="event">future</span>, we still need to think about it, design precautionary measures, and establish guidelines,” he writes. </p>
<p >That’s not to say there’s an expert consensus here — <a href="https://www.vox.com/future-perfect/2019/3/2/18244299/possible-minds-architects-intelligence-ai-experts">far from it</a>. There is substantial disagreement about which approaches seem likeliest to bring us to general <span class="concept">AI</span>, which approaches seem likeliest to bring us to <em>safe</em> general <span class="concept">AI</span>, and how soon we need to worry about any of this. </p>
<p >Many experts are wary that others are <a href="https://www.theverge.com/2018/10/16/17985168/deep-learning-revolution-terrence-sejnowski-artificial-intelligence-technology">overselling their field, and dooming it when the hype runs out</a>. But that disagreement shouldn’t obscure a growing common ground; these are possibilities worth thinking about, investing in, and researching, so we have guidelines when the moment comes that they’re needed.</p>

<h3 >5) Why couldn’t we just shut off a computer if it got too powerful?</h3>

<p >A smart <span class="concept">AI</span> could predict that we’d want to turn it off if it made us nervous. So it would try hard not to make us nervous, because doing so wouldn’t help it accomplish its goals. If asked what its intentions are, or what it’s working on, it would attempt to evaluate which responses are least likely to get it shut off, and answer with those. If it wasn’t competent enough to do that, it might pretend to be even dumber than it was — anticipating that <span class="people">researchers</span> would give it more <span class="concept">time</span>, computing resources, and training data.</p>
<p >So we might not know when it’s the right moment to shut off a computer. </p>
<p >We also might do things that make it impossible to shut off the computer later, even if we realize eventually that it’s a good idea. For example, many <span class="concept">AI</span> systems could have access to the internet, which is a rich source of training data and which they’d need if they’re to make money for their creators (for example, on the stock market, where <a href="https://www.cnbc.com/2017/06/13/death-of-the-human-investor-just-10-percent-of-trading-is-regular-stock-picking-jpmorgan-estimates.html">more than half of trading</a> is done by fast-reacting <span class="concept">AI</span> algorithms).</p>
<p >But with internet access, an <span class="concept">AI</span> could email copies of itself somewhere where they’ll be downloaded and read, or hack vulnerable systems elsewhere. Shutting off any one computer wouldn’t help.</p>
<p >In that case, isn’t it a terrible idea to let any <span class="concept">AI</span> system — even one which doesn’t seem powerful enough to be dangerous — have access to the internet? Probably. But that doesn’t mean it won’t continue to happen.<strong> </strong><span class="concept">AI</span> <span class="people">researchers</span> want to make their <span class="concept">AI</span> systems more capable — that’s what makes them more scientifically interesting and more profitable. It’s not clear that the many incentives to make your systems powerful and use them online will suddenly change once systems become powerful enough to be dangerous.</p>
<p >So far, we’ve mostly talked about the technical challenges of <span class="concept">AI</span>. But from here forward, it’s necessary to veer more into the politics. Since <span class="concept">AI</span> systems enable incredible things, there will be lots of different actors working on such systems. </p>
<p >There will likely be startups, established tech companies like <span class="place">Google</span> (Alphabet’s recently acquired startup <a href="https://deepmind.com/">DeepMind</a> is frequently mentioned as an <span class="concept">AI</span> frontrunner), and organizations like Elon-Musk-founded OpenAI, which <a href="https://www.vox.com/future-perfect/2019/4/17/18301070/openai-greg-brockman-ilya-sutskever">recently transitioned to a hybrid for-profit/non-profit structure</a>.</p>
<p >There will be governments — <span class="place">Russia</span>’s Vladimir Putin has expressed <a href="https://www.cnn.com/2017/09/01/world/putin-artificial-intelligence-will-rule-world/index.html">an interest in <span class="concept">AI</span></a>, and <span class="place">China</span> has made <a href="https://www.wired.com/story/ai-cold-war-china-could-doom-us-all/">big investments</a>. Some of them will presumably be cautious and employ safety measures, including keeping their <span class="concept">AI</span> off the internet. But in a scenario like this one, we’re at the mercy of the least cautious actor<em>, </em>whoever they may be. </p>
<p >That’s part of what makes <span class="concept">AI</span> hard: Even if we know how to take appropriate precautions (and right now we don’t), we also need to figure out how to ensure that all would-be <span class="concept">AI</span> programmers are motivated to take those precautions and have the tools to implement them correctly. </p>

<h3 >6) What are we doing right now to avoid an <span class="concept">AI</span> apocalypse? </h3>

<p >“It could be said that <span class="people">public</span> policy on AGI [artificial general intelligence] does not exist,” concluded a <a href="https://arxiv.org/pdf/1805.01109.pdf">paper in 2018<strong> </strong>reviewing the state of the field</a>. </p>
<p >The truth is that technical work on promising approaches is getting done, but there’s shockingly little in the way of policy planning, international collaboration, or <span class="people">public</span>-private partnerships. In fact, much of the work is being done by only a handful of organizations, and it has been estimated that around 50 people in the world work full <span class="concept">time</span> on technical <span class="concept">AI</span> safety.<strong> </strong></p>
<p >Bostrom’s <span class="event">future</span> of <span class="concept">humanity</span> Institute has published a <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/GovAIAgenda.pdf">research agenda for <span class="concept">AI</span> governance</a>: the study of “devising global norms, policies, and institutions to best ensure the beneficial development and use of advanced <span class="concept">AI</span>.” It has published research on the <a href="https://maliciousaireport.com/">risk of malicious uses of <span class="concept">AI</span></a>, on the <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_chinas_AI-Dream.pdf">context of <span class="place">China</span>’s <span class="concept">AI</span> strategy,</a> and on <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Artificial-Intelligence-and-International-Security-Syllabus.pdf">artificial intelligence and international security</a>. </p>
<p >The longest-established organization working on technical <span class="concept">AI</span> safety is the <a href="https://intelligence.org/">Machine Intelligence Research Institute</a> (MIRI), which prioritizes research into <a href="https://intelligence.org/technical-agenda/">designing highly reliable agents</a> — artificial intelligence programs whose behavior we can predict well enough to be confident they’re safe. (Disclosure: MIRI is a nonprofit and I donated to its work in<strong> </strong>2017-2019.)</p>
<p >The <span class="people">Elon Musk</span>-founded <a href="https://openai.com/">OpenAI </a>is a very new organization, less than three years old. But <span class="people">researchers</span> there are active contributors to both <span class="concept">AI</span> safety and <a href="https://openai.com/research/"><span class="concept">AI</span> capabilities</a> research. A research agenda in 2016 spelled out “<a href="https://arxiv.org/pdf/1606.06565.pdf">concrete open technical problems</a> relating to accident prevention in machine learning systems,” and <span class="people">researchers</span> have since <a href="https://arxiv.org/pdf/1810.08575.pdf">advanced some approaches to safe <span class="concept">AI</span> systems</a>.</p>
<p >Alphabet’s DeepMind, a leader in this field, has a safety team and a technical research agenda <a href="https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1">outlined here</a>. “Our intention is to ensure that <span class="concept">AI</span> systems of the <span class="event">future</span> are not just ‘hopefully safe’ but robustly, verifiably safe ,” it concludes, outlining an approach with an emphasis on specification (designing goals well), robustness (designing systems that perform within safe limits under volatile conditions), and assurance (monitoring systems and understanding what they’re doing). </p>
<p >There are also lots of people working on more present-day <span class="concept">AI</span> ethics problems: <a href="https://en.wikipedia.org/wiki/Algorithmic_bias">algorithmic bias</a>, <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of modern machine-learning algorithms to small changes, and transparency and interpretability of <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural nets</a>, to name just a few. Some of that research could potentially be valuable for preventing destructive scenarios. </p>
<p >But on the whole, the <a href="https://arxiv.org/pdf/1805.01109.pdf">state of the field</a> is a little bit as if almost all climate change <span class="people">researchers</span> were focused on managing the droughts, wildfires, and famines we’re already facing today, with only a tiny skeleton team dedicating to forecasting the <span class="event">future</span> and 50 or so <span class="people">researchers</span> who work full <span class="concept">time</span> on coming up with a <span class="concept">plan</span> to turn things around. </p>
<p >Not every organization with a major <span class="concept">AI</span> department has a safety team at all, and some of them have safety teams focused only on algorithmic fairness and not on the risks from advanced systems. The <span class="place">US</span> government doesn’t have a department for <span class="concept">AI</span>. </p>
<p >The field still has lots of open questions — many of which might make <span class="concept">AI</span> look much scarier, or much less so — which no one has dug into in depth. </p>

<h3 >7) Is this really likelier to kill us all than, say, climate change?</h3>

<p >It sometimes seems like we’re facing dangers from all angles in the 21st century. Both climate change and <span class="event">future</span> <span class="concept">AI</span> developments are likely to be transformative forces acting on our world. </p>
<p>Our predictions about climate change are more confident, both for better and for worse. We have a clearer understanding of the risks the planet will face, and we can estimate the costs to human civilization. They are projected to be enormous, risking potentially hundreds of millions of lives. The ones who will suffer most will be <a href="https://19january2017snapshot.epa.gov/climate-impacts/international-climate-impacts_.html">low-income people in developing countries</a>; the wealthy will find it easier to adapt. We also have a clearer understanding of the <a href="https://www.vox.com/energy-and-environment/2018/11/16/18096352/climate-change-clean-energy-policies-guide">policies we need to enact to address climate change</a> than we do with <span class="concept">AI</span>. </p>
  
  
<figure>
	<img src="img05.webp">
	</figure>

<p >There’s intense disagreement in the field<strong> </strong>on <a href="https://aiimpacts.org/predictions-of-human-level-ai-timelines/">timelines for critical advances in <span class="concept">AI</span></a>. While <span class="concept">AI</span> safety experts agree on many features of the safety problem, they’re still making the case to research teams in their own field, and they disagree on some of the details. There’s substantial disagreement on how badly it could go, and on how likely it is to go badly. There are only a few people who work full <span class="concept">time</span> on <span class="concept">AI</span> forecasting. One of the things current <span class="people">researchers</span> are trying to nail down is their models and the reasons for the remaining disagreements about what safe approaches will look like.</p>
<p >Most experts in the <span class="concept">AI</span> field think it poses a much larger risk of total human extinction than climate change, since analysts of existential risks to <span class="concept">Humanity</span> think that climate change, while catastrophic, is <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Existential-Risks-2017-01-23.pdf">unlikely to lead to human extinction</a>. But many others primarily emphasize our uncertainty — and emphasize that when we’re working rapidly toward powerful <span class="concept">technology</span> about which there are still many unanswered questions, the smart step is to start the research now.</p>

<h3 >8) Is there a possibility that <span class="concept">AI</span> can be benevolent? </h3>

<p ><span class="concept">AI</span> safety <span class="people">researchers</span> emphasize that we shouldn’t assume <span class="concept">AI</span> systems will be benevolent <a href="https://intelligence.org/2017/04/12/ensuring/">by default</a>. They’ll have the goals that their training environment set them up for, and no doubt this will fail to encapsulate the whole of human values. </p>
<p >When the <span class="concept">AI</span> gets smarter, might it figure out morality by itself? Again, <span class="people">researchers</span> emphasize that it won’t. It’s not really a matter of “figuring out” — the <span class="concept">AI</span> will understand just fine that humans actually value love and fulfillment and happiness, and not just the number associated with <span class="place">Google</span> on the New York Stock Exchange. But the <span class="concept">AI</span>’s values will be built around whatever goal system it was initially built around, which means it won’t suddenly become aligned with human values if it wasn’t designed that way to start with. </p>
<p >Of course, we can build <span class="concept">AI</span> systems that are aligned with human values, or at least that humans can safely work with. That is ultimately what almost every organization with an artificial general intelligence division is trying to do. Success with <span class="concept">AI</span> could give us access to decades or centuries of technological innovation all at once. </p>
<p >“If we’re successful, we believe this will be one of the most important and widely beneficial scientific advances ever made,” writes the <a href="https://deepmind.com/about/">introduction to Alphabet’s DeepMind</a>. “From climate change to the need for radically improved healthcare, too many problems suffer from painfully slow <span class="concept">progress</span>, their complexity overwhelming our ability to find <span class="concept">solutions</span>. With <span class="concept">AI</span> as a multiplier for human ingenuity, those <span class="concept">solutions</span> will come into reach.”</p>
<p>So, yes, <span class="concept">AI</span> can share our values — and transform our world for the good. We just need to solve a very hard engineering problem first.</p>

<h3 >9) I just really want to know: how worried should we be? </h3>

<p >To people who think the worrying is premature and the risks overblown, <span class="concept">AI</span> safety is competing with other priorities that sound, well, a bit less sci-fi — and it’s not clear why <span class="concept">AI</span> should take precedence. To people who think the risks described are real and substantial, it’s outrageous that we’re dedicating so few resources to working on them. </p>
<p >While machine-learning <span class="people">researchers</span> are right to be wary of hype, it’s also hard to avoid the fact that they’re accomplishing some impressive, surprising things using very generalizable techniques, and that it doesn’t seem that all the low-hanging fruit has been picked. </p>
<p> <span class="opinion"> <span class="concept">AI</span> looks increasingly like a <span class="concept">technology</span> that will change the world when it arrives </span>. <span class="people">Researchers</span> across many major <span class="concept">AI</span> organizations tell us it will be like <a href="https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1">launching a rocket</a>: something we <a href="https://intelligence.org/2018/10/03/rocket-alignment/">have to get right</a> before we hit “go.” So it seems urgent to get to work learning rocketry. No matter whether or not <span class="concept">Humanity</span> should be afraid, we should definitely be doing our homework.</p>

  </body>
</html>